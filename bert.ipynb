{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oumar/Projects/NLP/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import transformers\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>the cardiff roller collective roller sports le...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>pack fight song green bay first</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>the journal founded jesuit chaldean</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ajman international airport مطار عجمان upcomin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>kapla construction set children the sets consi...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           Sentence  Label\n",
       "0           0  the cardiff roller collective roller sports le...     11\n",
       "1           1                    pack fight song green bay first     11\n",
       "2           2                the journal founded jesuit chaldean     14\n",
       "3           3  ajman international airport مطار عجمان upcomin...      0\n",
       "4           4  kapla construction set children the sets consi...      4"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "def remove_stopwords(sentence):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = sentence.split(\" \")\n",
    "    filtered_sentence = [w.lower() for w in word_tokens if not w in stop_words and w.isalpha()]\n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "df['Sentence'] = df['Sentence'].apply(remove_stopwords)\n",
    "data = df['Sentence'].tolist()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random_seed = 42\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(random_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'cardiff',\n",
       " 'roller',\n",
       " 'collective',\n",
       " 'roller',\n",
       " 'sports',\n",
       " 'league',\n",
       " 'based',\n",
       " 'founded']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4290,  0.1327,  0.0481,  ..., -0.4838,  0.2110,  0.2386],\n",
      "         [-0.0974, -0.2360, -0.7596,  ..., -0.1143,  0.4335, -0.1886],\n",
      "         [ 0.2341,  0.1203,  0.6591,  ..., -0.3528, -0.5009, -1.0290],\n",
      "         ...,\n",
      "         [ 0.0519, -0.2148,  0.4049,  ...,  0.0147, -0.1591,  0.0069],\n",
      "         [-0.0186, -0.1655,  0.4328,  ..., -0.0134, -0.1543, -0.0319],\n",
      "         [-0.0638, -0.1615,  0.4873,  ..., -0.0411, -0.1506,  0.0063]],\n",
      "\n",
      "        [[-0.5071, -0.0117, -0.0223,  ..., -0.2034,  0.2611,  0.3132],\n",
      "         [ 0.1913, -0.1086,  0.1273,  ...,  0.0740, -0.0250,  0.0762],\n",
      "         [ 0.0119, -0.1807,  0.0075,  ...,  0.1420, -0.6898,  0.2468],\n",
      "         ...,\n",
      "         [-0.1719, -0.0975,  0.0186,  ...,  0.1558, -0.0718,  0.1889],\n",
      "         [-0.0336, -0.3679, -0.6167,  ...,  0.4977, -0.0801, -0.1862],\n",
      "         [-0.0523, -0.1249, -0.2135,  ...,  0.2645, -0.1796,  0.0445]],\n",
      "\n",
      "        [[-0.5199,  0.1083, -0.2523,  ..., -0.4810,  0.7192,  0.1198],\n",
      "         [-0.9432, -0.2891, -1.0047,  ...,  0.0558,  1.1876, -0.3851],\n",
      "         [-0.6440,  0.2901, -0.3276,  ..., -0.1971,  0.6695, -0.5934],\n",
      "         ...,\n",
      "         [-0.2573, -0.0649, -0.0771,  ...,  0.0180,  0.2470,  0.4007],\n",
      "         [-0.2803,  0.0198, -0.0849,  ...,  0.0685,  0.3108,  0.3620],\n",
      "         [-0.1810,  0.0108, -0.0725,  ..., -0.0348,  0.4091,  0.3937]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.6219, -0.4397, -0.5055,  ...,  0.1800,  0.0921, -0.1753],\n",
      "         [ 0.1527, -0.7972, -0.5429,  ..., -0.0174,  0.7733, -0.4372],\n",
      "         [-0.3746,  0.1468,  0.0137,  ..., -0.1998, -0.1290, -0.1323],\n",
      "         ...,\n",
      "         [-0.1225, -0.0802, -0.1855,  ..., -0.0427, -0.1969, -0.0265],\n",
      "         [-0.2975,  0.1499,  0.1674,  ..., -0.1171, -0.2899, -0.0556],\n",
      "         [-0.2867, -0.0999, -0.1073,  ...,  0.0858, -0.2396, -0.0427]],\n",
      "\n",
      "        [[-0.1976,  0.1868, -0.5457,  ..., -0.3735, -0.2030,  0.4227],\n",
      "         [ 0.0601,  0.5212, -0.4631,  ..., -0.7379, -0.0447, -0.4484],\n",
      "         [-0.2905,  0.9273, -0.7099,  ..., -0.5950, -0.9504, -0.8660],\n",
      "         ...,\n",
      "         [ 0.0529,  0.3433, -0.0584,  ..., -0.2982, -0.3200, -0.2871],\n",
      "         [ 0.0794,  0.2661, -0.0340,  ..., -0.2434, -0.3462, -0.2287],\n",
      "         [ 0.0859,  0.1840,  0.0254,  ..., -0.1965, -0.3261, -0.1588]],\n",
      "\n",
      "        [[-0.3889,  0.1712, -0.1321,  ..., -0.4139,  0.3500,  0.6097],\n",
      "         [-1.1489,  0.1052, -0.5540,  ...,  0.0447,  0.1100,  0.5432],\n",
      "         [-0.6170, -0.1844,  0.2234,  ...,  0.3588, -0.1546,  0.2596],\n",
      "         ...,\n",
      "         [-0.0391, -0.3747,  0.3116,  ...,  0.0033, -0.1956, -0.0109],\n",
      "         [-0.1044, -0.1836,  0.2363,  ..., -0.0610, -0.0434,  0.2131],\n",
      "         [-0.0989, -0.0852,  0.2863,  ..., -0.0594,  0.0161,  0.2883]]])\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.batch_encode_plus(\n",
    "            data,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "            add_special_tokens=True\n",
    ")\n",
    "\n",
    "input_ids = encoding['input_ids']\n",
    "attention_mask = encoding['attention_mask']\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    word_embeddings = outputs.last_hidden_state\n",
    "\n",
    "print(word_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "tokenized_text = tokenizer.tokenize(decoded_text)\n",
    "encoded_text = tokenizer.encode(data, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the tensor([[-0.4290,  0.1327,  0.0481,  ..., -0.4838,  0.2110,  0.2386],\n",
      "        [-0.0974, -0.2360, -0.7596,  ..., -0.1143,  0.4335, -0.1886],\n",
      "        [ 0.2341,  0.1203,  0.6591,  ..., -0.3528, -0.5009, -1.0290],\n",
      "        ...,\n",
      "        [ 0.0519, -0.2148,  0.4049,  ...,  0.0147, -0.1591,  0.0069],\n",
      "        [-0.0186, -0.1655,  0.4328,  ..., -0.0134, -0.1543, -0.0319],\n",
      "        [-0.0638, -0.1615,  0.4873,  ..., -0.0411, -0.1506,  0.0063]])\n",
      "cardiff tensor([[-0.5071, -0.0117, -0.0223,  ..., -0.2034,  0.2611,  0.3132],\n",
      "        [ 0.1913, -0.1086,  0.1273,  ...,  0.0740, -0.0250,  0.0762],\n",
      "        [ 0.0119, -0.1807,  0.0075,  ...,  0.1420, -0.6898,  0.2468],\n",
      "        ...,\n",
      "        [-0.1719, -0.0975,  0.0186,  ...,  0.1558, -0.0718,  0.1889],\n",
      "        [-0.0336, -0.3679, -0.6167,  ...,  0.4977, -0.0801, -0.1862],\n",
      "        [-0.0523, -0.1249, -0.2135,  ...,  0.2645, -0.1796,  0.0445]])\n",
      "roller tensor([[-0.5199,  0.1083, -0.2523,  ..., -0.4810,  0.7192,  0.1198],\n",
      "        [-0.9432, -0.2891, -1.0047,  ...,  0.0558,  1.1876, -0.3851],\n",
      "        [-0.6440,  0.2901, -0.3276,  ..., -0.1971,  0.6695, -0.5934],\n",
      "        ...,\n",
      "        [-0.2573, -0.0649, -0.0771,  ...,  0.0180,  0.2470,  0.4007],\n",
      "        [-0.2803,  0.0198, -0.0849,  ...,  0.0685,  0.3108,  0.3620],\n",
      "        [-0.1810,  0.0108, -0.0725,  ..., -0.0348,  0.4091,  0.3937]])\n",
      "collective tensor([[-0.6268,  0.1400, -0.2208,  ..., -0.3333,  0.6343,  0.4295],\n",
      "        [ 0.1147,  0.0967,  0.4668,  ..., -0.4325,  0.2661, -0.6779],\n",
      "        [-0.4726, -0.1009,  0.0262,  ..., -0.1103,  0.1608, -0.7741],\n",
      "        ...,\n",
      "        [-0.0812, -0.1658,  0.0548,  ...,  0.2262,  0.4169,  0.0513],\n",
      "        [-0.1211, -0.2805, -0.0127,  ...,  0.1275,  0.3225, -0.2676],\n",
      "        [ 0.0216,  0.1264,  0.1946,  ...,  0.2328,  0.2917, -0.7372]])\n",
      "roller tensor([[-0.3616,  0.1883, -0.1599,  ..., -0.4924,  0.0817,  0.7949],\n",
      "        [-0.5188, -0.6122, -0.4083,  ..., -0.3845, -0.1956,  0.3644],\n",
      "        [-0.1942,  0.1606,  0.4463,  ..., -0.2516, -1.0137, -0.3844],\n",
      "        ...,\n",
      "        [-0.1443,  0.0765, -0.2119,  ..., -0.2281, -0.1948,  0.4403],\n",
      "        [-0.0810,  0.0711, -0.2304,  ..., -0.2512, -0.2618,  0.4400],\n",
      "        [-0.0373,  0.0181, -0.1975,  ..., -0.2556, -0.2741,  0.3873]])\n",
      "sports tensor([[-6.5072e-01,  3.7081e-01, -9.5145e-02,  ..., -2.7103e-01,\n",
      "          6.7010e-01,  3.0782e-01],\n",
      "        [-1.2912e+00,  3.6144e-01,  2.0223e-02,  ..., -6.5141e-01,\n",
      "          4.1978e-01,  5.3555e-01],\n",
      "        [-2.4593e-01,  2.6645e-01,  4.5914e-01,  ..., -3.5552e-01,\n",
      "         -3.2032e-01, -9.8320e-01],\n",
      "        ...,\n",
      "        [-5.7625e-01,  3.9944e-01, -1.2860e-01,  ..., -9.9362e-02,\n",
      "          5.2946e-02, -1.1397e-01],\n",
      "        [-6.1809e-01,  1.1055e-01, -3.7062e-01,  ...,  5.1935e-02,\n",
      "          7.8805e-01, -1.4070e-01],\n",
      "        [-3.4392e-01,  2.7983e-01,  6.8351e-04,  ..., -9.0186e-02,\n",
      "         -6.3453e-02, -2.6798e-01]])\n",
      "league tensor([[-0.1972,  0.2149, -0.1239,  ..., -0.2183,  0.1940,  0.2083],\n",
      "        [-0.2249,  0.0290, -1.1367,  ...,  0.3458,  0.5538,  0.2648],\n",
      "        [-0.5030,  0.5673, -0.1065,  ...,  0.0421, -0.6604, -0.3492],\n",
      "        ...,\n",
      "        [ 0.2439, -0.1390, -0.3317,  ...,  0.0391,  0.2018, -0.0427],\n",
      "        [ 0.2462, -0.0982, -0.3682,  ...,  0.0313,  0.2749, -0.0359],\n",
      "        [ 0.1592,  0.0274, -0.3440,  ...,  0.0831,  0.2274,  0.0663]])\n",
      "based tensor([[-0.8476, -0.0331, -0.4200,  ..., -0.2060,  0.1608,  0.2157],\n",
      "        [ 0.0477, -0.6096,  0.0743,  ..., -0.3929,  0.1874,  0.2054],\n",
      "        [-0.3478, -0.6810,  0.6026,  ...,  0.4349,  0.2114,  0.5108],\n",
      "        ...,\n",
      "        [-0.2134,  0.1786,  0.1641,  ..., -0.1845,  0.0132, -0.0799],\n",
      "        [-0.2802,  0.1844,  0.1942,  ..., -0.2367,  0.0041, -0.0823],\n",
      "        [-0.1779,  0.1141,  0.0030,  ..., -0.1900, -0.0141,  0.0129]])\n",
      "founded tensor([[-0.4946, -0.0418, -0.2173,  ..., -0.1127, -0.0866,  0.1446],\n",
      "        [-0.0638, -0.0741,  0.4165,  ...,  0.2378, -0.2072, -0.6576],\n",
      "        [ 0.0074, -0.1052,  0.2965,  ..., -0.3353, -0.3600, -0.5441],\n",
      "        ...,\n",
      "        [-0.0311,  0.1993, -0.0478,  ..., -0.0425, -0.4003,  0.0478],\n",
      "        [-0.1060,  0.0814,  0.0879,  ...,  0.0180, -0.2877,  0.0937],\n",
      "        [-0.1161,  0.1637,  0.1615,  ...,  0.0089, -0.3112,  0.1147]])\n"
     ]
    }
   ],
   "source": [
    "for token, embedding in zip(tokenized_text, word_embeddings):\n",
    "    print(token, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 the\n",
      "1 cardiff\n",
      "2 roller\n",
      "3 collective\n",
      "4 roller\n",
      "5 sports\n",
      "6 league\n",
      "7 based\n",
      "8 founded\n"
     ]
    }
   ],
   "source": [
    "for i, token_str in enumerate(tokenized_text):\n",
    "    print(i, token_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get word embeddings (did this for the cosine similarity)\n",
    "def get_word_embeddings(sentences, tokenizer, model):\n",
    "    word_embeddings = {}\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        for word in words:\n",
    "            if word not in word_embeddings:\n",
    "                inputs = tokenizer(word, \n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    return_tensors='pt',\n",
    "                    add_special_tokens=True)\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**inputs)\n",
    "                word_embeddings[word] = outputs.last_hidden_state[0][0].numpy()\n",
    "    return word_embeddings\n",
    "\n",
    "# Get word embeddings for all words in the dataset\n",
    "word_embeddings = get_word_embeddings(data, tokenizer, model)\n",
    "words = list(word_embeddings.keys())\n",
    "embeddings = np.array([word_embeddings[word] for word in words]).astype('float32')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'born' and 'Arabic': 0.8113988041877747\n"
     ]
    }
   ],
   "source": [
    "# compute cosine similarity \n",
    "def cosine_similarity(embedding1, embedding2):\n",
    "    similarity = cosine_similarity([embedding1], [embedding2])\n",
    "    return similarity[0][0]\n",
    "\n",
    "word1 = 'born'\n",
    "word2 = 'Arabic'\n",
    "\n",
    "# Retrieve embeddings \n",
    "embedding1 = word_embeddings.get(word1)\n",
    "embedding2 = word_embeddings.get(word2)\n",
    "\n",
    "if embedding1 is not None and embedding2 is not None:\n",
    "    similarity = cosine_similarity(embedding1, embedding2)\n",
    "    print(f\"Similarity between '{word1}' and '{word2}': {similarity}\")\n",
    "else:\n",
    "    missing_words = [word for word in [word1, word2] if word not in word_embeddings]\n",
    "    print(f\"Embeddings not found: {', '.join(missing_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['born', 'received', 'represented', 'served', 'known', 'created', 'accepted', 'named', 'person', 'announced']\n"
     ]
    }
   ],
   "source": [
    "# Faiss\n",
    "import faiss\n",
    "\n",
    "d = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(embeddings)\n",
    "\n",
    "def faiss_search(word, word_embeddings, index, words, k=5):\n",
    "    if word not in word_embeddings:\n",
    "        return f\"Embedding for '{word}' not found.\"\n",
    "    embedding = np.array([word_embeddings[word]]).astype('float32')\n",
    "    d, I = index.search(embedding, k)\n",
    "    similar_words = [words[i] for i in I[0]]\n",
    "    return similar_words\n",
    "\n",
    "print(faiss_search('born', word_embeddings, index, words, k=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arabic', 'english', 'egyptian', 'kurdish', 'spanish', 'albanian', 'french', 'urdu', 'greek', 'danish']\n"
     ]
    }
   ],
   "source": [
    "print(faiss_search('arabic', word_embeddings, index, words, k=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
