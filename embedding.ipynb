{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Sentence\n",
      "0  The Cardiff Roller Collective (CRoC) are a rol...\n",
      "1  \"Go! Pack Go!\" is the fight song of the Green ...\n",
      "2  Al-Machriq (English translation: The East) was...\n",
      "3  Ajman International Airport (Arabic: مطار عجما...\n",
      "4  Kapla is a construction set for children and a...\n"
     ]
    }
   ],
   "source": [
    "data = load_files('wkp_sorted')\n",
    "\n",
    "sentences = []\n",
    "\n",
    "for file in data.filenames:\n",
    "    with open(file, 'r') as f:\n",
    "        words = []\n",
    "        for line in f:\n",
    "            words.extend(line.split())\n",
    "        first_15 = words[:15]\n",
    "        sentence = ' '.join(first_15)\n",
    "        sentences.append(sentence)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(sentences, columns=['Sentence'])\n",
    "df.to_csv('data.csv')\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The Cardiff Roller Collective (CRoC) are a rol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\"Go! Pack Go!\" is the fight song of the Green ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Al-Machriq (English translation: The East) was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Ajman International Airport (Arabic: مطار عجما...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Kapla is a construction set for children and a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           Sentence\n",
       "0           0  The Cardiff Roller Collective (CRoC) are a rol...\n",
       "1           1  \"Go! Pack Go!\" is the fight song of the Green ...\n",
       "2           2  Al-Machriq (English translation: The East) was...\n",
       "3           3  Ajman International Airport (Arabic: مطار عجما...\n",
       "4           4  Kapla is a construction set for children and a..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word2Vec\n",
    "# from scipy.linalg.special_matrices import triu\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The Cardiff Roller Collective CRoC roller spor...</td>\n",
       "      <td>[the, cardiff, roller, collective, croc, rolle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Go Pack Go fight song Green Bay Packers first</td>\n",
       "      <td>[go, pack, go, fight, song, green, bay, packer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>English translation The East journal founded J...</td>\n",
       "      <td>[english, translation, the, east, journal, fou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Ajman International Airport Arabic مطار عجمان ...</td>\n",
       "      <td>[ajman, international, airport, arabic, مطار, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Kapla construction set children adults The set...</td>\n",
       "      <td>[kapla, construction, set, children, adults, t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           Sentence  \\\n",
       "0           0  The Cardiff Roller Collective CRoC roller spor...   \n",
       "1           1      Go Pack Go fight song Green Bay Packers first   \n",
       "2           2  English translation The East journal founded J...   \n",
       "3           3  Ajman International Airport Arabic مطار عجمان ...   \n",
       "4           4  Kapla construction set children adults The set...   \n",
       "\n",
       "                                           Tokenized  \n",
       "0  [the, cardiff, roller, collective, croc, rolle...  \n",
       "1  [go, pack, go, fight, song, green, bay, packer...  \n",
       "2  [english, translation, the, east, journal, fou...  \n",
       "3  [ajman, international, airport, arabic, مطار, ...  \n",
       "4  [kapla, construction, set, children, adults, t...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Remove stopwords and punctuation\n",
    "def remove_stopwords(sentence):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(sentence)\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words and w.isalpha()]\n",
    "    # remove numbers\n",
    "    filtered_sentence = [w for w in filtered_sentence if not w.isdigit()]\n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "\n",
    "data['Sentence'] = data['Sentence'].apply(remove_stopwords)\n",
    "data.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "d = []\n",
    "for sentence in data['Sentence']:\n",
    "    tok = []\n",
    "    for word in word_tokenize(sentence):\n",
    "        tok.append(word.lower())\n",
    "    d.append(tok)\n",
    "\n",
    "\n",
    "data['Tokenized'] = d\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyedVectors<vector_size=100, 975 keys>\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(data['Tokenized'], window=5, min_count=1, workers=4)\n",
    "word_vectors = model.wv\n",
    "model.save('word2vec.bin')\n",
    "print(model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=975, vector_size=100, alpha=0.025>\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec.load('word2vec.bin')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Tokenized</th>\n",
       "      <th>Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The Cardiff Roller Collective CRoC roller spor...</td>\n",
       "      <td>[the, cardiff, roller, collective, croc, rolle...</td>\n",
       "      <td>[-0.0012059944, 0.00057655765, 0.0004546229, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Go Pack Go fight song Green Bay Packers first</td>\n",
       "      <td>[go, pack, go, fight, song, green, bay, packer...</td>\n",
       "      <td>[0.0023387286, 0.0013184368, 0.0018480197, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>English translation The East journal founded J...</td>\n",
       "      <td>[english, translation, the, east, journal, fou...</td>\n",
       "      <td>[-0.0027212065, 0.0029319907, 0.002298687, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Ajman International Airport Arabic مطار عجمان ...</td>\n",
       "      <td>[ajman, international, airport, arabic, مطار, ...</td>\n",
       "      <td>[0.00021810588, 0.001563381, 0.0033868733, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Kapla construction set children adults The set...</td>\n",
       "      <td>[kapla, construction, set, children, adults, t...</td>\n",
       "      <td>[0.00047397893, 0.0011637352, 0.003106683, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           Sentence  \\\n",
       "0           0  The Cardiff Roller Collective CRoC roller spor...   \n",
       "1           1      Go Pack Go fight song Green Bay Packers first   \n",
       "2           2  English translation The East journal founded J...   \n",
       "3           3  Ajman International Airport Arabic مطار عجمان ...   \n",
       "4           4  Kapla construction set children adults The set...   \n",
       "\n",
       "                                           Tokenized  \\\n",
       "0  [the, cardiff, roller, collective, croc, rolle...   \n",
       "1  [go, pack, go, fight, song, green, bay, packer...   \n",
       "2  [english, translation, the, east, journal, fou...   \n",
       "3  [ajman, international, airport, arabic, مطار, ...   \n",
       "4  [kapla, construction, set, children, adults, t...   \n",
       "\n",
       "                                                Mean  \n",
       "0  [-0.0012059944, 0.00057655765, 0.0004546229, 0...  \n",
       "1  [0.0023387286, 0.0013184368, 0.0018480197, 0.0...  \n",
       "2  [-0.0027212065, 0.0029319907, 0.002298687, -0....  \n",
       "3  [0.00021810588, 0.001563381, 0.0033868733, -0....  \n",
       "4  [0.00047397893, 0.0011637352, 0.003106683, -0....  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean of word vectors\n",
    "def mean_vector(words, model):\n",
    "    # remove out of vocabulary words\n",
    "    words = [word for word in words if word in model.wv]\n",
    "    if len(words) >= 1:\n",
    "        return np.mean(model.wv[words], axis=0)\n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "data['Mean'] = data['Tokenized'].apply(lambda x: mean_vector(x, model))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-03 09:44:25.183322: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-03 09:44:25.872984: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-03 09:44:26.901204: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-03 09:44:27.577377: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-03 09:44:27.581029: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-03 09:44:29.040701: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-03 09:45:21.240919: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 ['the', 'wide', 'road', 'shimmered', 'in', 'the', 'hot', 'sun']\n"
     ]
    }
   ],
   "source": [
    "#  Vetorize a sample\n",
    "sen = \"The wide road shimmered in the hot sun\"\n",
    "tokens = list(sen.lower().split())\n",
    "print(len(tokens), tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save mapping fron token to integer indices\n",
    "import collections\n",
    "\n",
    "tokens2int = collections.defaultdict(lambda: len(tokens2int))\n",
    "tokens2int['<pad>'] # add a padding token\n",
    "token_ids = [tokens2int[token] for token in tokens]\n",
    "vocab_size = (len(tokens2int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, 'the': 1, 'wide': 2, 'road': 3, 'shimmered': 4, 'in': 5, 'hot': 6, 'sun': 7}\n"
     ]
    }
   ],
   "source": [
    "vocab, index = {}, 1  # start indexing from 1\n",
    "vocab['<pad>'] = 0  # add a padding token\n",
    "for token in tokens:\n",
    "  if token not in vocab:\n",
    "    vocab[token] = index\n",
    "    index += 1\n",
    "vocab_size = len(vocab)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<pad>', 1: 'the', 2: 'wide', 3: 'road', 4: 'shimmered', 5: 'in', 6: 'hot', 7: 'sun'}\n"
     ]
    }
   ],
   "source": [
    "int2tokens = {index: token for token, index in vocab.items()}\n",
    "print(int2tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "# Generate skip-grams\n",
    "window_size = 2\n",
    "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "      token_ids, \n",
    "      vocabulary_size=vocab_size,\n",
    "      window_size=window_size,\n",
    "      negative_samples=0)\n",
    "print(len(positive_skip_grams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 7): (hot, sun)\n",
      "(1, 3): (the, road)\n",
      "(5, 1): (in, the)\n",
      "(5, 3): (in, road)\n",
      "(3, 2): (road, wide)\n"
     ]
    }
   ],
   "source": [
    "for target, context in positive_skip_grams[:5]:\n",
    "  print(f\"({target}, {context}): ({int2tokens[target]}, {int2tokens[context]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2 1 4 3], shape=(4,), dtype=int64)\n",
      "['wide', 'the', 'shimmered', 'road']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-03 10:36:33.765235: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-03 10:36:33.766990: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# Negative\n",
    "# Get target and context words for one positive skip-gram\n",
    "target_word, context_word = positive_skip_grams[0]\n",
    "\n",
    "# Set the number of negative samples per positive context\n",
    "num_ns = 4\n",
    "\n",
    "context_class = tf.reshape(tf.constant(context_word, dtype=\"int64\"), (1, 1))\n",
    "negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "    true_classes=context_class,  \n",
    "    num_true=1, \n",
    "    num_sampled=num_ns,  \n",
    "    unique=True,  \n",
    "    range_max=vocab_size,  \n",
    "    seed=SEED,  \n",
    "    name=\"negative_sampling\"  \n",
    ")\n",
    "\n",
    "print(negative_sampling_candidates)\n",
    "print([int2tokens[index.numpy()] for index in negative_sampling_candidates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce a dimension so you can use concatenation (in the next step).\n",
    "squeezed_context_class = tf.squeeze(context_class, 1)\n",
    "\n",
    "# Concatenate a positive context word with negative sampled words.\n",
    "context = tf.concat([squeezed_context_class, negative_sampling_candidates], 0)\n",
    "\n",
    "# Label the first context word as `1` (positive) followed by `num_ns` `0`s (negative).\n",
    "label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "target = target_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_index    : 6\n",
      "target_word     : hot\n",
      "context_indices : [7 2 1 4 3]\n",
      "context_words   : ['sun', 'wide', 'the', 'shimmered', 'road']\n",
      "label           : [1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"target_index    : {target}\")\n",
    "print(f\"target_word     : {int2tokens[target_word]}\")\n",
    "print(f\"context_indices : {context}\")\n",
    "print(f\"context_words   : {[int2tokens[c.numpy()] for c in context]}\")\n",
    "print(f\"label           : {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00315225 0.00315225 0.00547597 0.00741556 0.00912817 0.01068435\n",
      " 0.01212381 0.01347162 0.01474487 0.0159558 ]\n"
     ]
    }
   ],
   "source": [
    "sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(size=10)\n",
    "print(sampling_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
